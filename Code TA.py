# -*- coding: utf-8 -*-
"""TA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NqQjsAsHalFUMUv8jeMBLXX5gxkeqLP4

# Preprocessing Data Text

## Import Dataset
"""

import pandas as pd

# Read the CSV file with the correct delimiter and show the first few rows
df = pd.read_csv('combined_comments.csv', delimiter=',')
df

import pandas as pd

df['Sentiment']
# Misalkan y adalah kolom yang berisi label kelas
class_distribution = df['Sentiment'].value_counts()
print(class_distribution)

"""## Text Cleaning"""

import re

def tokens_cleansing(tokens):
    # Remove Emoticon
    tokens = re.sub(r'(?::|;|=)(?:-)?(?:\)|\(|D|P|V|v|p|d)', '', tokens)
    # Remove usernames (words starting with @)
    tokens = re.sub(r'@\w+', '', tokens)
    # Remove Hashtag (words starting with #)
    tokens = re.sub(r'#\w+', '', tokens)
    # Remove Username
    tokens = re.sub(r'USERNAME', '', tokens)
    # Remove RT
    tokens = re.sub(r'RT', '', tokens)
    # Remove URLs
    tokens = re.sub(r'http\S+|www\S+|https\S+', '', tokens, flags=re.MULTILINE)
    # Remove special characters, symbols, etc. (keeping only letters and spaces)
    tokens = re.sub(r'[^A-Za-z\s]', '', tokens)
    tokens = re.sub(r'[0-9]+',' ',tokens)
    tokens = tokens.strip(' ')

    return tokens

def preprocess_tweet(text):
    return tokens_cleansing(text)

# Apply preprocessing to the 'Text Tweet' column
df['clean_text'] = df['Comment Text'].apply(preprocess_tweet)

# Display the first few rows to see the changes
df

"""## Tokenizing"""

!pip install nltk

import nltk

nltk.download('punkt')

from nltk.tokenize import word_tokenize
def tokenize_text(text):
    return word_tokenize(text)

df['clean_text'] = df['clean_text'].apply(tokenize_text)

df

"""## Case Folding"""

def casefolding(tokens):
  return [token.lower() for token in tokens]

df['clean_text'] = df['clean_text'].apply(casefolding)

df

"""## Mengubah Slang Word"""

# Load the Indonesian slang words dictionary
slang_dict_df = pd.read_csv('merged_slang.csv')

# Convert the slang dictionary dataframe to a dictionary
slang_dict = dict(zip(slang_dict_df.iloc[:, 0], slang_dict_df.iloc[:, 1]))

def replace_slang(text, slang_dict):
    return [slang_dict[word] if word in slang_dict else word for word in text if word.strip()]

# Apply the slang replacement to the 'Cleaned Text' column
df['clean_text'] = df['clean_text'].apply(lambda x: replace_slang(x, slang_dict))

df

"""## Stopword Removal"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

def remove_stopwords(tokens):
    stop_words = set(stopwords.words('indonesian'))
    return [token for token in tokens if token.lower() not in stop_words]

df['clean_text'] = df['clean_text'].apply(remove_stopwords)

df

"""## Stemming"""

!pip install PySastrawi

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
import pandas as pd

# Membuat stemmer
factory = StemmerFactory()
stemmer = factory.create_stemmer()

# Fungsi untuk melakukan stemming pada teks
def stem_text(tokens):
    return [stemmer.stem(token) for token in tokens if token.strip()]

# Misalkan 'tweets_df' adalah DataFrame yang sudah memiliki kolom 'Cleaned Text'
df['clean_text'] = df['clean_text'].apply(stem_text)

# Tampilkan hasilnya
df

"""## Penggabungan Processed Text"""

# Menggabungkan kata-kata
df['Processed Text'] = df['clean_text'].apply(lambda x: ' '.join(x))
df

"""## Pembobotan Teks (TF-IDF)"""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(df['Processed Text'])

# Print TF-IDF matrix
tfidf_df = pd.DataFrame(tfidf_matrix.todense().T,
                         index=vectorizer.get_feature_names_out(),
                         columns=[f'D{i+1}' for i in range(len(df))])
tfidf_df.style.set_caption("TF-IDF Matrix")

features = vectorizer.get_feature_names_out()
print(f"Total Fitur: {len(features)}")

"""# Klasifikasi Tanpa Fitur Seleksi PSO"""

import pandas as pd
from sklearn import model_selection
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, recall_score, precision_score, f1_score, confusion_matrix
import numpy as np

# Persiapan data
y = df['Sentiment'].apply(lambda x: 1 if x == 'positive' else 0)
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)

# Inisialisasi KFold
kf = KFold(n_splits=10, shuffle=True, random_state=42)

# List untuk menyimpan hasil setiap fold
results = []
predictions = []

# Inisialisasi confusion matrix kumulatif
num_classes = len(np.unique(y_encoded))
cumulative_confusion_matrix = np.zeros((num_classes, num_classes))

# Loop untuk k-fold validation
for train_index, test_index in kf.split(tfidf_matrix):
    X_train, X_test = tfidf_matrix[train_index], tfidf_matrix[test_index]
    y_train, y_test = y_encoded[train_index], y_encoded[test_index]

    # Training model
    model = MultinomialNB()
    model.fit(X_train, y_train)

    # Prediksi pada data test
    y_test_pred = model.predict(X_test)

    # Prediksi pada data train
    y_train_pred = model.predict(X_train)

    # Menghitung metrik pada data test
    test_accuracy = accuracy_score(y_test, y_test_pred)
    test_precision = precision_score(y_test, y_test_pred, average='weighted')
    test_recall = recall_score(y_test, y_test_pred, average='weighted')
    test_f1 = f1_score(y_test, y_test_pred, average='weighted')

    # Menghitung metrik pada data train
    train_accuracy = accuracy_score(y_train, y_train_pred)

    # Menyimpan hasil dalam dictionary
    fold_result = {
        'Fold': len(results) + 1,
        'Train Accuracy': train_accuracy * 100,
        'Test Accuracy': test_accuracy * 100,
        'Test Precision': test_precision * 100,
        'Test Recall': test_recall * 100,
        'Test F1 Score': test_f1 * 100,
    }
    results.append(fold_result)

    # Simpan prediksi dan nilai sebenarnya dalam DataFrame bersama teks aslinya
    fold_predictions = pd.DataFrame({
        'Fold': len(results),
        'Original Text': df.iloc[test_index]['Processed Text'].values,
        'True Label': y_test,
        'Predicted Label': y_test_pred
    })
    predictions.append(fold_predictions)

    # Menghitung confusion matrix untuk fold ini
    fold_confusion_matrix = confusion_matrix(y_test, y_test_pred)

    # Menambahkan confusion matrix dari fold ini ke cumulative matrix
    cumulative_confusion_matrix += fold_confusion_matrix

    print(f"Laporan Klasifikasi Fold ke-{len(results)} (Test Data):\n", classification_report(y_test, y_test_pred))

# Mengonversi hasil ke DataFrame
results_df = pd.DataFrame(results)

# Menggabungkan semua prediksi menjadi satu DataFrame
predictions_df = pd.concat(predictions, ignore_index=True)

# Menampilkan hasil dalam tabel
print(results_df)

# Menampilkan rata-rata akurasi dari semua fold
print("Rata-rata Akurasi:", np.mean(results_df['Test Accuracy']), "%")

# Menampilkan tabel prediksi
predictions_df

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Plot cumulative confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cumulative_confusion_matrix, annot=True, fmt='g', cmap='Blues', cbar=True)

# Menambahkan label sumbu
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Cumulative Confusion Matrix')

# Menambahkan label kelas pada sumbu x dan y (opsional)
class_labels = encoder.classes_  # Jika Anda memiliki label kelas yang telah di-encode
plt.xticks(np.arange(len(class_labels)) + 0.5, class_labels, rotation=45, ha='right')
plt.yticks(np.arange(len(class_labels)) + 0.5, class_labels, rotation=0)

plt.show()

import numpy as np

# Example cumulative confusion matrix from all folds
cumulative_confusion_matrix = np.array([[361, 99], [44, 609]])

# Extract true positives, false positives, false negatives, and true negatives
tn, fp, fn, tp = cumulative_confusion_matrix.ravel()

# Calculate overall metrics
accuracy = (tp + tn) / (tp + tn + fp + fn)
precision = tp / (tp + fp)
recall = tp / (tp + fn)
f1 = 2 * (precision * recall) / (precision + recall)

print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')

"""## Analisis Tanpa PSO"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Mengubah generator menjadi list
kf_split = list(kf.split(tfidf_matrix))

# Fold ke-6
fold_index6 = 5
train_index6, test_index6 = kf_split[fold_index6]

# Mendapatkan data train untuk fold ke-6
X_train_fold6 = tfidf_matrix[train_index6]
y_train_fold6 = y_encoded[train_index6]

# Menghitung rata-rata nilai TF-IDF untuk setiap fitur berdasarkan kelas
mean_features_class0 = X_train_fold6[y_train_fold6 == 0].mean(axis=0).A1
mean_features_class1 = X_train_fold6[y_train_fold6 == 1].mean(axis=0).A1

# Membuat DataFrame untuk memudahkan analisis
feature_names = vectorizer.get_feature_names_out()
df_features = pd.DataFrame({
    'Feature': feature_names,
    'Mean_Class_0': mean_features_class0,
    'Mean_Class_1': mean_features_class1,
    'Difference': mean_features_class1 - mean_features_class0
})

# Sortir berdasarkan perbedaan terbesar
df_features_sorted = df_features.iloc[np.argsort(np.abs(df_features['Difference']))[::-1]]

# Menampilkan 10 fitur teratas yang paling berbeda
print(df_features_sorted.head(10))

# Visualisasi 10 fitur teratas berdasarkan perbedaan rata-rata antara kelas
feature_difference = mean_features_class1 - mean_features_class0
top_features = np.argsort(np.abs(feature_difference))[-10:]

plt.figure(figsize=(10, 6))
plt.barh(range(10), feature_difference[top_features])
plt.yticks(range(10), np.array(vectorizer.get_feature_names_out())[top_features])
plt.xlabel("Perbedaan Rata-rata TF-IDF")
plt.title("10 Fitur Teratas Berdasarkan Perbedaan Rata-rata antara Kelas (Fold ke-6)")
plt.show()

import numpy as np
import pandas as pd

# Mengubah generator menjadi list
kf_split = list(kf.split(tfidf_matrix))

# Fold ke-1
fold_index1 = 1
train_index1, test_index1 = kf_split[fold_index1]

# Mendapatkan data train untuk fold ke-1
X_train_fold1 = tfidf_matrix[train_index1]
y_train_fold1 = y_encoded[train_index1]

# Menghitung rata-rata nilai TF-IDF untuk setiap fitur berdasarkan kelas
mean_features_class0 = X_train_fold1[y_train_fold1 == 0].mean(axis=0).A1
mean_features_class1 = X_train_fold1[y_train_fold1 == 1].mean(axis=0).A1

# Membuat DataFrame untuk memudahkan analisis
feature_names = vectorizer.get_feature_names_out()
df_features = pd.DataFrame({
    'Feature': feature_names,
    'Mean_Class_0': mean_features_class0,
    'Mean_Class_1': mean_features_class1,
    'Difference': mean_features_class1 - mean_features_class0
})

# Sortir berdasarkan perbedaan terbesar
df_features_sorted = df_features.iloc[np.argsort(np.abs(df_features['Difference']))[::-1]]

# Menampilkan 10 fitur teratas yang paling berbeda
print(df_features_sorted.head(10))

# Visualisasi 10 fitur teratas berdasarkan perbedaan rata-rata antara kelas
feature_difference = mean_features_class1 - mean_features_class0
top_features = np.argsort(np.abs(feature_difference))[-10:]

plt.figure(figsize=(10, 6))
plt.barh(range(10), feature_difference[top_features])
plt.yticks(range(10), np.array(vectorizer.get_feature_names_out())[top_features])
plt.xlabel("Perbedaan Rata-rata TF-IDF")
plt.title("10 Fitur Teratas Berdasarkan Perbedaan Rata-rata antara Kelas (Fold ke-2)")
plt.show()

"""#Optimasi PSO dengan Dynamic Inertia Weight

## PSO
"""

import time
import numpy as np
from sklearn.model_selection import KFold
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

class Particle:
    def __init__(self, dim, lb, ub, Vmax, Vmin):
        # Inisialisasi posisi dan kecepatan partikel secara acak
        self.position = np.random.uniform(lb, ub, dim)
        self.velocity = np.random.uniform(Vmin, Vmax, dim)
        # Simpan posisi terbaik dan biaya terbaik
        self.best_position = self.position.copy()
        self.best_cost = float('inf')
        self.cost = float('inf')

    def update_velocity(self, w, c1, c2, global_best_position):
        # Komponen acak untuk pembaruan kecepatan
        r1 = np.random.rand(len(self.position))
        r2 = np.random.rand(len(self.position))
        cognitive_component = c1 * r1 * (self.best_position - self.position)
        social_component = c2 * r2 * (global_best_position - self.position)
        # Update kecepatan dengan inertia weight, cognitive, dan social component
        self.velocity = w * self.velocity + cognitive_component + social_component

    def update_position(self, lb, ub):
        # Update posisi partikel
        self.position = self.position + self.velocity
        # Memastikan posisi berada dalam batas
        self.position = np.clip(self.position, lb, ub)

class PSOFeatureSelectionDynamic:
    def __init__(self, n_particles, max_iter, w_start, w_end, c1, c2, lb, ub, thres, dim):
        self.n_particles = n_particles
        self.max_iter = max_iter
        self.w_start = w_start
        self.w_end = w_end
        self.c1 = c1
        self.c2 = c2
        self.lb = np.full(dim, lb)
        self.ub = np.full(dim, ub)
        self.Vmax = (self.ub - self.lb) / 2
        self.Vmin = -self.Vmax
        self.thres = thres
        self.global_best_position = None
        self.global_best_cost = float('inf')
        # Inisialisasi partikel
        self.particles = [Particle(dim, self.lb, self.ub, self.Vmax, self.Vmin) for _ in range(n_particles)]
        self.convergence_curve = []

    def binary_conversion(self, position):
        # Konversi posisi kontinu ke biner
        return np.where(position > self.thres, 1, 0)

    def objective_function(self, xtrain, ytrain, x, params):
        alpha = 0.99
        beta = 1 - alpha
        # Memilih fitur berdasarkan posisi biner
        selected_features = np.where(x == 1)[0]
        num_feat = len(selected_features)
        max_feat = len(x)

        if num_feat == 0:
            return 1  # Biaya terburuk jika tidak ada fitur yang dipilih

        xtrain_selected = xtrain[:, selected_features]
        model = MultinomialNB()
        model.fit(xtrain_selected, ytrain)

        fold = params['fold']
        xtest_selected = fold['xv'][:, selected_features]
        ytest = fold['yv']

        y_pred = model.predict(xtest_selected)
        accuracy = accuracy_score(ytest, y_pred)

        error = 1 - accuracy
        cost = alpha * error + beta * (num_feat / max_feat)
        return cost

    def optimize(self, xtrain, ytrain, params):
        for t in range(self.max_iter):
            # Update dynamic inertia weight
            w = self.w_start - (self.w_start - self.w_end) * (t / self.max_iter)
            for particle in self.particles:
                xbin = self.binary_conversion(particle.position)
                particle.cost = self.objective_function(xtrain, ytrain, xbin, params)

                if particle.cost < particle.best_cost:
                    particle.best_cost = particle.cost
                    particle.best_position = particle.position.copy()

                if particle.cost < self.global_best_cost:
                    self.global_best_cost = particle.cost
                    self.global_best_position = particle.position.copy()

                # Update velocity dan posisi setelah evaluasi fitness
                particle.update_velocity(w, self.c1, self.c2, self.global_best_position)
                particle.update_position(self.lb, self.ub)

            # Simpan fitness global terbaik untuk analisis konvergensi
            self.convergence_curve.append(self.global_best_cost)
            print(f"Iteration: {t + 1}, Best Fitness: {self.global_best_cost}")

        xbin = self.binary_conversion(self.global_best_position)
        selected_features = np.where(xbin == 1)[0]
        return {'sf': selected_features, 'c': self.convergence_curve, 'nf': len(selected_features)}

"""## Finding Best Parameter"""

import time
import numpy as np
from sklearn.model_selection import KFold, ParameterSampler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

# Initialize X & Y
X = tfidf_matrix
y = df['Sentiment'].apply(lambda x: 1 if x == 'positive' else 0)

# Define parameter distributions for random search
param_distributions = {
    'c1': [1.5, 2.0],
    'c2': [2.0, 2.5],
    'n_particles': [50, 100],
    'max_iter': [100, 150]
}

n_iter_search = 20
random_search = list(ParameterSampler(param_distributions, n_iter=n_iter_search, random_state=42))

best_params = None
best_score = float('-inf')

kf = KFold(n_splits=10, shuffle=True, random_state=42)

for params in random_search:
    scores = []
    for train_index, test_index in kf.split(X):
        xtrain, xtest = X[train_index], X[test_index]
        ytrain, ytest = y[train_index], y[test_index]

        fold_params = {'fold': {'xt': xtrain, 'yt': ytrain, 'xv': xtest, 'yv': ytest}}

        pso = PSOFeatureSelectionDynamic(n_particles=params['n_particles'], max_iter=params['max_iter'], w_start=0.9, w_end=0.4, c1=params['c1'], c2=params['c2'], lb=0, ub=1, thres=0.5, dim=X.shape[1])

        start_time = time.time()
        pso_result = pso.optimize(xtrain, ytrain, fold_params)
        print("Waktu Running --- %s Detik ---" % (time.time() - start_time))

        selected_features = pso_result['sf']
        xtrain_selected = xtrain[:, selected_features]
        xtest_selected = xtest[:, selected_features]

        model = MultinomialNB()
        model.fit(xtrain_selected, ytrain)
        y_pred = model.predict(xtest_selected)

        score = accuracy_score(ytest, y_pred)
        scores.append(score)

    mean_score = np.mean(scores)
    if mean_score > best_score:
        best_score = mean_score
        best_params = params

print(f"Best Parameters: {best_params} with Mean Score: {best_score}")

"""## Running PSO"""

import time
import numpy as np
from sklearn.model_selection import KFold
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score
import pandas as pd
import matplotlib.pyplot as plt
import pickle

# Initialize X & Y
X = tfidf_matrix
y = df['Sentiment'].apply(lambda x: 1 if x == 'positive' else 0)

# Initialize KFold
kf = KFold(n_splits=10, shuffle=True, random_state=42)

# List untuk menyimpan hasil setiap fold
results = []
predictions = []

fold = 1

for train_index, test_index in kf.split(X):
    xtrain, xtest = X[train_index], X[test_index]
    ytrain, ytest = y[train_index], y[test_index]

    fold_params = {'fold': {'xt': xtrain, 'yt': ytrain, 'xv': xtest, 'yv': ytest}}

    # Perform Feature Selection with PSO
    pso = PSOFeatureSelectionDynamic(
        n_particles=50, max_iter=150, w_start=0.9, w_end=0.4, c1=2.0, c2=2.0, lb=0, ub=1, thres=0.5, dim=X.shape[1]
    )
    start_time = time.time()
    pso_result = pso.optimize(xtrain, ytrain, fold_params)
    print("Waktu Running --- %s Detik ---" % (time.time() - start_time))

    selected_features = pso_result['sf']
    curve = pso_result['c']
    num_selected_features = pso_result['nf']

    # Train Naive Bayes using selected features
    xtrain_selected = xtrain[:, selected_features]
    xtest_selected = xtest[:, selected_features]

    model = MultinomialNB()
    model.fit(xtrain_selected, ytrain)

    # Predict and evaluate
    y_pred = model.predict(xtest_selected)
    accuracy = accuracy_score(ytest, y_pred)
    precision = precision_score(ytest, y_pred)
    recall = recall_score(ytest, y_pred)
    f1 = f1_score(ytest, y_pred)

    # Simpan hasil dalam dictionary
    fold_result = {
        'Fold': fold,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1,
        'Selected Feature': selected_features,
        'Num Features': num_selected_features
    }
    results.append(fold_result)

    # Simpan prediksi dan nilai sebenarnya dalam DataFrame bersama teks aslinya
    fold_predictions = pd.DataFrame({
        'Fold': fold,
        'Original Text': df.iloc[test_index]['Processed Text'].values,
        'True Label': ytest,
        'Predicted Label': y_pred
    })
    predictions.append(fold_predictions)

    print("Laporan Klasifikasi:\n", classification_report(ytest, y_pred))
    print(f"Fold {fold} Accuracy: {accuracy*100:.2f}%")

    # Feature Selected
    print(f"Total Feature Selected: {num_selected_features}")

    # Curve Fitness
    plt.plot(curve)
    plt.xlabel('Iterations')
    plt.ylabel('Best Fitness')
    plt.title(f'Convergence Curve for Fold {fold}')
    plt.show()
    fold += 1

# Mengonversi hasil ke DataFrame
results_df = pd.DataFrame(results)

# Menggabungkan semua prediksi menjadi satu DataFrame
predictions_df_PSO = pd.concat(predictions, ignore_index=True)

# Menampilkan hasil dalam tabel
print(results_df[['Fold', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Num Features']])

# Menampilkan rata-rata akurasi dari semua fold
mean_accuracy = np.mean(results_df['Accuracy'])
print(f"Mean Accuracy: {mean_accuracy*100:.2f}%")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Menggabungkan semua prediksi menjadi satu DataFrame (dari kode sebelumnya)
predictions_df_PSO = pd.concat(predictions, ignore_index=True)

# Membuat confusion matrix berdasarkan semua hasil prediksi
cm = confusion_matrix(predictions_df_PSO['True Label'], predictions_df_PSO['Predicted Label'])

# Menampilkan confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for All Folds')
plt.show()

# Menampilkan tabel hasil prediksi dengan teks asli
predictions_df_PSO

from sklearn.metrics import classification_report

# Menghitung classification report
report = classification_report(predictions_df_PSO['True Label'], predictions_df_PSO['Predicted Label'])

# Menampilkan classification report
print("Classification Report:\n", report)

"""## Analisis dengan PSO"""

import numpy as np
import pandas as pd

# Mendapatkan selected features untuk fold ke-1
fold_1_selected_features = results_df.loc[results_df['Fold'] == 1, 'Selected Feature'].values[0]

# Mendapatkan data train untuk fold ke-1
kf_split = list(kf.split(X))
train_index1, test_index1 = kf_split[0]  # Fold ke-1 memiliki index ke-0 dalam list
X_train_fold1 = X[train_index1][:, fold_1_selected_features]
y_train_fold1 = y[train_index1]

# Ambil nama fitur yang dipilih oleh PSO
selected_feature_names_fold1 = np.array(vectorizer.get_feature_names_out())[fold_1_selected_features]

# Menghitung rata-rata nilai TF-IDF untuk setiap fitur berdasarkan kelas
mean_features_class0_fold1 = X_train_fold1[y_train_fold1 == 0].mean(axis=0).A1
mean_features_class1_fold1 = X_train_fold1[y_train_fold1 == 1].mean(axis=0).A1

# Membuat DataFrame untuk memudahkan analisis
df_features_fold1 = pd.DataFrame({
    'Feature': selected_feature_names_fold1,
    'Mean_Class_0': mean_features_class0_fold1,
    'Mean_Class_1': mean_features_class1_fold1,
    'Difference': mean_features_class1_fold1 - mean_features_class0_fold1
})

# Sortir berdasarkan perbedaan terbesar
df_features_sorted_fold1 = df_features_fold1.iloc[np.argsort(np.abs(df_features_fold1['Difference']))[::-1]]

# Menampilkan 10 fitur teratas yang paling berbeda
print(df_features_sorted_fold1.head(10))

# Visualisasi 10 fitur teratas berdasarkan perbedaan rata-rata antara kelas
feature_difference_fold1 = df_features_sorted_fold1['Difference'].values
top_features_fold1 = np.argsort(np.abs(feature_difference_fold1))[-10:]

plt.figure(figsize=(10, 6))
plt.barh(range(10), feature_difference_fold1[top_features_fold1])
plt.yticks(range(10), df_features_sorted_fold1['Feature'].values[top_features_fold1])
plt.xlabel("Perbedaan Rata-rata TF-IDF")
plt.title("10 Fitur Teratas Berdasarkan Perbedaan Rata-rata antara Kelas (Fold ke-1)")
plt.show()

import numpy as np
import pandas as pd

# Mendapatkan selected features untuk fold ke-2
fold_2_selected_features = results_df.loc[results_df['Fold'] == 2, 'Selected Feature'].values[0]

# Mendapatkan data train untuk fold ke-2
kf_split = list(kf.split(X))
train_index2, test_index2 = kf_split[1]  # Fold ke-2 memiliki index ke-1 dalam list
X_train_fold2 = X[train_index2][:, fold_2_selected_features]
y_train_fold2 = y[train_index2]

# Ambil nama fitur yang dipilih oleh PSO
selected_feature_names_fold2 = np.array(vectorizer.get_feature_names_out())[fold_2_selected_features]

# Menghitung rata-rata nilai TF-IDF untuk setiap fitur berdasarkan kelas
mean_features_class0_fold2 = X_train_fold2[y_train_fold2 == 0].mean(axis=0).A1
mean_features_class1_fold2 = X_train_fold2[y_train_fold2 == 1].mean(axis=0).A1

# Membuat DataFrame untuk memudahkan analisis
df_features_fold2 = pd.DataFrame({
    'Feature': selected_feature_names_fold2,
    'Mean_Class_0': mean_features_class0_fold2,
    'Mean_Class_1': mean_features_class1_fold2,
    'Difference': mean_features_class1_fold2 - mean_features_class0_fold2
})

# Sortir berdasarkan perbedaan terbesar
df_features_sorted_fold2 = df_features_fold2.iloc[np.argsort(np.abs(df_features_fold2['Difference']))[::-1]]

# Menampilkan 10 fitur teratas yang paling berbeda
print(df_features_sorted_fold2.head(10))

# Visualisasi 10 fitur teratas berdasarkan perbedaan rata-rata antara kelas
feature_difference_fold2 = df_features_sorted_fold2['Difference'].values
top_features_fold2 = np.argsort(np.abs(feature_difference_fold2))[-10:]

plt.figure(figsize=(10, 6))
plt.barh(range(10), feature_difference_fold2[top_features_fold2])
plt.yticks(range(10), df_features_sorted_fold2['Feature'].values[top_features_fold2])
plt.xlabel("Perbedaan Rata-rata TF-IDF")
plt.title("10 Fitur Teratas Berdasarkan Perbedaan Rata-rata antara Kelas (Fold ke-2)")
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Mengubah generator menjadi list
kf_split = list(kf.split(X))
train_index1, test_index1 = kf_split[0]

# Mendapatkan data train dan test untuk fold ke-1
X_train_fold1 = X[train_index1][:, fold_1_selected_features]
X_test_fold1 = X[test_index1][:, fold_1_selected_features]
y_train_fold1 = y[train_index1]
y_test_fold1 = y[test_index1]

# Train Naive Bayes menggunakan selected features untuk fold ke-1
model = MultinomialNB()
model.fit(X_train_fold1, y_train_fold1)

# Mendapatkan nilai sebenarnya dan prediksi pada fold ke-1
y_pred_fold1 = model.predict(X_test_fold1)

# Buat confusion matrix
cm = confusion_matrix(y_test_fold1, y_pred_fold1)
accuracy = accuracy_score(y_test_fold1, y_pred_fold1)
print(f"Fold 1 Accuracy: {accuracy*100:.2f}%")

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title(f'Confusion Matrix for Fold 1')
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Mengubah generator menjadi list
kf_split = list(kf.split(X))
train_index2, test_index2 = kf_split[1]

# Mendapatkan data train dan test untuk fold ke-2
X_train_fold2 = X[train_index2][:, fold_2_selected_features]
X_test_fold2 = X[test_index2][:, fold_2_selected_features]
y_train_fold2 = y[train_index2]
y_test_fold2 = y[test_index2]

# Train Naive Bayes menggunakan selected features untuk fold ke-2
model = MultinomialNB()
model.fit(X_train_fold2, y_train_fold2)

# Mendapatkan nilai sebenarnya dan prediksi pada fold ke-2
y_pred_fold2 = model.predict(X_test_fold2)

# Buat confusion matrix
cm = confusion_matrix(y_test_fold2, y_pred_fold2)
accuracy = accuracy_score(y_test_fold2, y_pred_fold2)
print(f"Fold 2 Accuracy: {accuracy*100:.2f}%")

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title(f'Confusion Matrix for Fold 2')
plt.show()

# Mengubah generator menjadi list
kf_split = list(kf.split(X))

# Ambil train_index dan test_index untuk fold ke-1 (misalnya)
train_index1, test_index1 = kf_split[0]

# Teks asli dari data uji
text = df.iloc[test_index1[0]]['Processed Text']

# Fitur yang dipilih oleh PSO
selected_feature_names = np.array(vectorizer.get_feature_names_out())[fold_1_selected_features]

# Menampilkan teks asli sebelum seleksi fitur
print("Teks asli sebelum seleksi fitur:")
print(text)

# Memfilter teks untuk hanya menampilkan kata-kata yang dipilih oleh PSO
filtered_text = ' '.join([word for word in text.split() if word in selected_feature_names])

# Menampilkan teks setelah seleksi fitur
print("\nTeks setelah seleksi fitur (hanya fitur yang dipilih oleh PSO):")
print(filtered_text)